{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping mine data with `.apply`\n",
    "\n",
    "## The pages we'll be looking at\n",
    "\n",
    "If I wanted to read specific information about a specfic mine, it takes a few steps. **Do these steps with your browser before you try any programming.**\n",
    "\n",
    "1. Visit the [Mine Data Retrieval System](https://arlweb.msha.gov/drs/drshome.htm)\n",
    "2. Scroll down to **Mine Identification Number (ID) Search**\n",
    "3. Type in a mine ID number, such as `3503598`, click **Search**\n",
    "4. I'm on a page! It lists the MINE NAME and MINE OWNER.\n",
    "\n",
    "After searching for and finding a mine, I can use this page to **find reports about this mine**. Some of the reports are on accidents, violations, inspections, health samples and more. To get those reports:\n",
    "\n",
    "1. Search for a mine (if you haven't already)\n",
    "2. Scroll down and change **Beginning Date** to `1/1/1995` (violation reports begin in 1995, accidents begin in 1983)\n",
    "3. Select the report type of `Violations`\n",
    "4. Click **Get Report**\n",
    "5. I'm on a page! It lists ALL OF THE MINE'S VIOLATIONS.\n",
    "\n",
    "By changing the report type you're searching for you can find all sorts of different data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing this programmatically\n",
    "\n",
    "## First, scraping a single page\n",
    "\n",
    "### Import your imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for a mine\n",
    "\n",
    "Visit the [Mine Data Retrieval System](https://arlweb.msha.gov/drs/drshome.htm) and use Selenium to search for `3503598`\n",
    "\n",
    "- *TIP: You might need to use the Selenium code to scroll down to the right spot on the page. Or not!*\n",
    "- *TIP: Use `.send_keys` to type into the box*\n",
    "- *TIP: On pages that never change, you can usually just use XPath if you're feeling lazy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://arlweb.msha.gov/drs/drshome.htm')\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchfld = driver.find_element_by_name('MineId')\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true)\", searchfld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchfld.send_keys(3503598)\n",
    "time.sleep(1)\n",
    "searchfld.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding reports\n",
    "\n",
    "On the \"Report Selection Page\" (where you should be after you search), use Selenium to...\n",
    "\n",
    "- Change the **Beginning Date** to `1/1/1995`\n",
    "- Select the report type of `Violations`\n",
    "- Click **Get Report**\n",
    "\n",
    ".\n",
    "\n",
    "- *TIP: Remember, if someone isn't on the page Selenium can't click it!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_input = driver.find_element_by_name('BDate')\n",
    "date_input.send_keys('1/1/1995')\n",
    "# select violations radio button by xpath\n",
    "driver.find_element_by_xpath('//*[@id=\"content\"]/form[1]/table[3]/tbody/tr[2]/td[2]/table/tbody/tr[1]/td/input').click()\n",
    "date_input.send_keys(Keys.RETURN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_raw = driver.find_elements_by_tag_name('tr')\n",
    "# I could've gone for .find_elements_by_class_name('drsviols') as well.\n",
    "# My approach is probably less organized but also much less typing.\n",
    "violation_lines = [vio.text.split() for vio in info_raw[31:] if vio.text != None] \n",
    "violation_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving reports\n",
    "\n",
    "Save all of the rows of data on that page into a new dataframe. Each column is its own column, **and you also need to save the URL under the 'Standard' column.** Here, I even made you a blank dictionary:\n",
    "\n",
    "```python\n",
    "data = {}\n",
    "data['violator'] = ''\n",
    "data['contract_id'] = ''\n",
    "data['citation_no'] = ''\n",
    "data['case_no'] = ''\n",
    "data['date_issues'] = ''\n",
    "data['final_order_date'] = ''\n",
    "data['section_of_act'] = ''\n",
    "data['date_terminated'] = ''\n",
    "data['citation'] = ''\n",
    "data['s_and_s'] = ''\n",
    "data['standard'] = ''\n",
    "data['standard_url'] = ''\n",
    "data['proposed_penalty'] = ''\n",
    "data['citation_status'] = ''\n",
    "data['current_penalty'] = ''\n",
    "data['amount_paid'] = ''\n",
    "```\n",
    "\n",
    "- *TIP: Some of those table rows aren't what you want. How can you tell them apart from the good ones? (the previous mine owner ones are okay, I just mean the weird headers)*\n",
    "- *TIP: I sense `.find_elements` + a lot of square brackets*\n",
    "- *TIP: This is just like scraping a search results page!*\n",
    "- *TIP: For the URL, you'll need to find the `a` inside of the cell*\n",
    "- *TIP: class name is sadly not going to save your life here, because some of the `tr`s and `td`s have the same class! It's stupid. But there's a trick: CSS selectors! Something like `div#container` finds a `div` with the id of `container`, while `span.important` finds a `span` with the class of `important`. It should be helpful! And use `.find_elements_by_` + tab to see what the command is*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_raw = driver.find_elements_by_tag_name('tr')\n",
    "# The split is making things complicated for now, but I can grab every item whenever I want\n",
    "# The [31:] cuts of the header and stuff above the data\n",
    "violation_lines = [vio.text.split() for vio in info_raw[31:] if vio.text != ''] \n",
    "all_datas =[]\n",
    "for violine in violation_lines:\n",
    "\n",
    "    data = {}\n",
    "    try:\n",
    "        data['violator'] = ' '.join(violine[0:-13])\n",
    "        # Will take care of this later, for now all are empty\n",
    "        data['contract_id'] = np.nan\n",
    "        data['citation_no'] = violine[-13]\n",
    "        data['case_no'] = violine[-12]\n",
    "        data['date_issues'] = violine[-11]\n",
    "        data['final_order_date'] = violine[-10]\n",
    "        data['section_of_act'] = violine[-9]\n",
    "        data['date_terminated'] = violine[-8]\n",
    "        data['citation'] = violine[-7]\n",
    "        data['s_and_s'] = violine[-6]\n",
    "        data['standard'] = violine[-5]\n",
    "        # The url thingy is very lazy, I know...\n",
    "        data['standard_url'] = driver.find_element_by_partial_link_text(violine[-5]).get_attribute('href') \n",
    "        data['proposed_penalty'] = violine[-4]\n",
    "        data['citation_status'] = violine[-3]\n",
    "        data['current_penalty'] = violine[-2]\n",
    "        data['amount_paid'] = violine[-1]\n",
    "    except:\n",
    "        print('ERROR:',violine)\n",
    "    #print(data)\n",
    "    all_datas.append(data)\n",
    "\n",
    "all_datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving that data\n",
    "\n",
    "Save the dataframe to a CSV file called `3503598-violations.csv` (that's the TDLR code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_datas)\n",
    "# There is one empty cell left over from the table headers:\n",
    "df.dropna(thresh = 10, inplace=True) \n",
    "df.head(5)\n",
    "df.to_csv('3503598-violations.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put that all in ONE cell that runs correctly\n",
    "\n",
    "The **entire process**, from searching to saving as a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://arlweb.msha.gov/drs/drshome.htm')\n",
    "time.sleep(3)\n",
    "\n",
    "searchfld = driver.find_element_by_name('MineId')\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true)\", searchfld)\n",
    "searchfld.send_keys(3503598)\n",
    "time.sleep(1)\n",
    "searchfld.send_keys(Keys.RETURN)\n",
    "\n",
    "date_input = driver.find_element_by_name('BDate')\n",
    "date_input.send_keys('1/1/1995')\n",
    "driver.find_element_by_xpath('//*[@id=\"content\"]/form[1]/table[3]/tbody/tr[2]/td[2]/table/tbody/tr[1]/td/input').click()\n",
    "date_input.send_keys(Keys.RETURN)\n",
    "\n",
    "info_raw = driver.find_elements_by_tag_name('tr')\n",
    "violation_lines = [vio.text.split() for vio in info_raw[28:] if vio.text != ''] \n",
    "all_datas =[]\n",
    "errorcount = 0\n",
    "for violine in violation_lines:\n",
    "\n",
    "    data = {}\n",
    "    try:\n",
    "        data['violator'] = ' '.join(violine[0:-13])\n",
    "        data['contract_id'] = np.nan\n",
    "        data['citation_no'] = violine[-13]\n",
    "        data['case_no'] = violine[-12]\n",
    "        data['date_issues'] = violine[-11]\n",
    "        data['final_order_date'] = violine[-10]\n",
    "        data['section_of_act'] = violine[-9]\n",
    "        data['date_terminated'] = violine[-8]\n",
    "        data['citation'] = violine[-7]\n",
    "        data['s_and_s'] = violine[-6]\n",
    "        data['standard'] = violine[-5]\n",
    "        # The url thingy is very lazy, I know...\n",
    "        data['standard_url'] = driver.find_element_by_partial_link_text(violine[-5]).get_attribute('href') \n",
    "        data['proposed_penalty'] = violine[-4]\n",
    "        data['citation_status'] = violine[-3]\n",
    "        data['current_penalty'] = violine[-2]\n",
    "        data['amount_paid'] = violine[-1]\n",
    "    except:\n",
    "        # This is for debugging only. With most mine data sets, there are 4 errors from the header rows.\n",
    "        errorcount = errorcount + 1\n",
    "    all_datas.append(data)\n",
    "\n",
    "\n",
    "print(errorcount)\n",
    "df = pd.DataFrame(all_datas)\n",
    "# There is a bit of noise. I drop it here:\n",
    "df.dropna(thresh = 12, inplace=True) \n",
    "driver.close()\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using .apply to find data about SEVERAL mines\n",
    "\n",
    "The file `mines-subset.csv` has a list of mine IDs. We're going to scrape the operator's name for each of those mines.\n",
    "\n",
    "### Open up `mines-subset.csv` and save it into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mines-subset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open up `mines-subset.csv` in a text editor, then look at your dataframe. Is something different about them? If so, make them match.\n",
    "\n",
    "- *TIP: You can zero fill if you want, but another option is that when reading in a CSV, `dtype='str'` will force everything to be a string*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mines-subset.csv', dtype = 'str')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert your one-cell scraper into a function, and use it on each row of our dataset\n",
    "\n",
    "- *TIP: You'll be using `.apply`*\n",
    "- *TIP: You won't be joining this back into your dataframe, so you don't need to `return` anything or `join` or any of that.*\n",
    "- *TIP: Be careful of your **other variable names** - if you're calling the thing you're sending your function `row`, you can't use it anywhere else (like in your loop)*\n",
    "- *TIP: **BE CAREFUL WHAT YOU NAME YOUR DATAFRAMES.** If you name the citations dataframe `df` it can overwrite your mine ID `df`*\n",
    "- *TIP: You'll be saving a dataframe each time*\n",
    "- *TIP: Be sure you change everything that refers to the mine ID to refer to the current row's ID instead of `3503598`*\n",
    "- *TIP: BE SURE TO CHANGE EVERYTHING THAT REFERS TO THE MINE ID*\n",
    "- *TIP: EVERYTHING, EVERYTHING, EVERYTHING! Look at the end of your function! Maybe I'm overreacting, I don't know.*\n",
    "- *TIP: If you hit an error about list index out of range, see what line it's happening on and go look at the page. What's different about this page than the previous ones? (answer: the last three columns!) If you assign those columns later using `try`/`except` you should be able to get some data from those rows without throwing it all out. If you can't figure it out, just wrap it all in try/except and give up on those rows*\n",
    "- *TIP: Some of the standards might not have links, either, so you might want to wrap that in a `try`/`except`, too!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Okay, now do it for ALL of the mines\n",
    "\n",
    "Open up `mines.csv` using pandas and do the same thing, it will just be for more mines this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
